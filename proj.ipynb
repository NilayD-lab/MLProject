{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "# run this in cmd to avoid an error beforehand for this import\n",
    "# python -m nltk.downloader stopwords \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN       deeds reason earthquake may allah forgive us   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  residents asked shelter place notified officer...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randNoun(lines):\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "    if (len(nouns) == 0): # if no nouns picked up return rand word in text\n",
    "        return lines[random.randrange(len(lines))]\n",
    "    return nouns[random.randrange(len(nouns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of the dataset\n",
    "print(len(df[\"location\"]))\n",
    "words = {} # will hold unique keywords\n",
    "count = 0\n",
    "# getting rid of %20 in keyword and getting a list of all the unqiue keywords\n",
    "for i in df[\"keyword\"]:\n",
    "    i = str(i).replace(\"%20\", ' ')\n",
    "    if (i not in words.keys() and i != \"nan\"):\n",
    "        words[i] = count\n",
    "        count+=1\n",
    "\n",
    "# filling in missing feature values\n",
    "# missing keywords get filled in with a keywords already in \"words\" if that keyword shows up in text, otherwise pick a random word from text\n",
    "# Missing \"location\" gets filled in with \"Earth\"\n",
    "for i,j in df.iterrows():\n",
    "    if (pd.isna(j[\"keyword\"])):\n",
    "        for k in words.keys():\n",
    "            if k in j[\"text\"]:\n",
    "                df.at[i, \"keyword\"] = k\n",
    "            else:\n",
    "                df.at[i, \"keyword\"] = randNoun(j[\"text\"])\n",
    "    if (pd.isna(j[\"location\"])):\n",
    "        df.at[i, \"location\"] = \"Earth\"\n",
    "# for i, j in df.iterrows():\n",
    "#     if (pd.isna(j[\"location\"])):\n",
    "#         df.drop(i, axis=0, inplace=True)\n",
    "\n",
    "print(len(df[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(inplace=True, drop=True)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'earthquake': 0, 'sask': 1, 'residents': 2, 'people': 3, 'alaska': 4, 'rockyfire': 5, 'rain': 6, 'hill': 7, 'theres': 8, 'area': 9, 'south': 10, 'days': 11, 'myanmar': 12, 'crash': 13, 'whats': 14, 'fruits': 15, 'summer': 16, 'car': 17, 'goooooooaaaaaal': 18, 'u': 19, 'london': 20, 'love': 21, 'day': 22, 'looooool': 23, 'wayi': 24, 'week': 25, 'girlfriend': 26, 'cooool': 27, 'pasta': 28, 'end': 29, 'ablaze': 30, 'accident': 31, 'aftershock': 32, 'airplane%20accident': 33, 'ambulance': 34, 'annihilated': 35, 'annihilation': 36, 'apocalypse': 37, 'armageddon': 38, 'army': 39, 'arson': 40, 'arsonist': 41, 'attack': 42, 'attacked': 43, 'avalanche': 44, 'battle': 45, 'bioterror': 46, 'bioterrorism': 47, 'blaze': 48, 'blazing': 49, 'bleeding': 50, 'blew%20up': 51, 'blight': 52, 'blizzard': 53, 'blood': 54, 'bloody': 55, 'blown%20up': 56, 'body%20bag': 57, 'body%20bagging': 58, 'body%20bags': 59, 'bomb': 60, 'bombed': 61, 'bombing': 62, 'bridge%20collapse': 63, 'buildings%20burning': 64, 'buildings%20on%20fire': 65, 'burned': 66, 'burning': 67, 'burning%20buildings': 68, 'bush%20fires': 69, 'casualties': 70, 'casualty': 71, 'catastrophe': 72, 'catastrophic': 73, 'chemical%20emergency': 74, 'cliff%20fall': 75, 'collapse': 76, 'collapsed': 77, 'collide': 78, 'collided': 79, 'collision': 80, 'crashed': 81, 'crush': 82, 'crushed': 83, 'curfew': 84, 'cyclone': 85, 'damage': 86, 'danger': 87, 'dead': 88, 'death': 89, 'deaths': 90, 'debris': 91, 'deluge': 92, 'deluged': 93, 'demolish': 94, 'demolished': 95, 'demolition': 96, 'derail': 97, 'derailed': 98, 'derailment': 99, 'desolate': 100, 'desolation': 101, 'destroy': 102, 'destroyed': 103, 'destruction': 104, 'detonate': 105, 'detonation': 106, 'devastated': 107, 'devastation': 108, 'disaster': 109, 'displaced': 110, 'drought': 111, 'drown': 112, 'drowned': 113, 'drowning': 114, 'dust%20storm': 115, 'electrocute': 116, 'electrocuted': 117, 'emergency': 118, 'emergency%20plan': 119, 'emergency%20services': 120, 'engulfed': 121, 'epicentre': 122, 'evacuate': 123, 'evacuated': 124, 'evacuation': 125, 'explode': 126, 'exploded': 127, 'explosion': 128, 'eyewitness': 129, 'famine': 130, 'fatal': 131, 'fatalities': 132, 'fatality': 133, 'fear': 134, 'fire': 135, 'fire%20truck': 136, 'first%20responders': 137, 'flames': 138, 'flattened': 139, 'flood': 140, 'flooding': 141, 'floods': 142, 'forest%20fire': 143, 'forest%20fires': 144, 'hail': 145, 'hailstorm': 146, 'harm': 147, 'hazard': 148, 'hazardous': 149, 'heat%20wave': 150, 'hellfire': 151, 'hijack': 152, 'hijacker': 153, 'hijacking': 154, 'hostage': 155, 'hostages': 156, 'hurricane': 157, 'injured': 158, 'injuries': 159, 'injury': 160, 'inundated': 161, 'inundation': 162, 'landslide': 163, 'lava': 164, 'lightning': 165, 'loud%20bang': 166, 'mass%20murder': 167, 'mass%20murderer': 168, 'massacre': 169, 'mayhem': 170, 'meltdown': 171, 'military': 172, 'mudslide': 173, 'natural%20disaster': 174, 'nuclear%20disaster': 175, 'nuclear%20reactor': 176, 'obliterate': 177, 'obliterated': 178, 'obliteration': 179, 'oil%20spill': 180, 'outbreak': 181, 'pandemonium': 182, 'panic': 183, 'panicking': 184, 'police': 185, 'quarantine': 186, 'quarantined': 187, 'radiation%20emergency': 188, 'rainstorm': 189, 'razed': 190, 'refugees': 191, 'rescue': 192, 'rescued': 193, 'rescuers': 194, 'riot': 195, 'rioting': 196, 'rubble': 197, 'ruin': 198, 'sandstorm': 199, 'screamed': 200, 'screaming': 201, 'screams': 202, 'seismic': 203, 'sinkhole': 204, 'sinking': 205, 'siren': 206, 'sirens': 207, 'smoke': 208, 'snowstorm': 209, 'storm': 210, 'stretcher': 211, 'structural%20failure': 212, 'suicide%20bomb': 213, 'suicide%20bomber': 214, 'suicide%20bombing': 215, 'sunk': 216, 'survive': 217, 'survived': 218, 'survivors': 219, 'terrorism': 220, 'terrorist': 221, 'threat': 222, 'thunder': 223, 'thunderstorm': 224, 'tornado': 225, 'tragedy': 226, 'trapped': 227, 'trauma': 228, 'traumatised': 229, 'trouble': 230, 'tsunami': 231, 'twister': 232, 'typhoon': 233, 'upheaval': 234, 'violent%20storm': 235, 'volcano': 236, 'war%20zone': 237, 'weapon': 238, 'weapons': 239, 'whirlwind': 240, 'wild%20fires': 241, 'wildfire': 242, 'windstorm': 243, 'wounded': 244, 'wounds': 245, 'wreck': 246, 'wreckage': 247, 'wrecked': 248, 'pic': 249, 'explodingkittens\\x89û': 250, 'road': 251, 'sismo': 252, 'responsibility': 253, '\\x89ûïhannaph\\x89û\\x9d': 254, 'dei': 255, 'headquarters': 256, 'everyone': 257, 'thing': 258, 'pm': 259, 'issues': 260, 'reunion': 261, 'app': 262, 'roosevelt': 263, 'spill': 264, 'warning': 265, 'ebola': 266, 'passengers': 267, 'site': 268, 'stormchase': 269, 'homes': 270, 'troubling': 271, 'm194': 272, 'ebike': 273, 'california': 274}\n"
     ]
    }
   ],
   "source": [
    "# we are going to feature transform \n",
    "corpus = df[\"text\"].to_list() # combine all text to a bag of words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray() # TF-IDF text encoding\n",
    "\n",
    "#doing hot and cold encoding for keywords\n",
    "words = {} # will hold unique keywords with a corresponding index\n",
    "count = 0\n",
    "for i in df[\"keyword\"]:\n",
    "    if (i not in words.keys()):\n",
    "        words[i] = count\n",
    "        count+=1\n",
    "print(words)\n",
    "keywords_hot_cold = np.zeros((len(df[\"keyword\"]), len(words.keys()))) # init feature shape\n",
    "for i in range(keywords_hot_cold.shape[0]):\n",
    "    row = np.zeros(keywords_hot_cold.shape[1])\n",
    "    row[words[df.at[i, \"keyword\"]]] = 1 # using the index found in the dict, index the row and set the corresponding value of the keyword to 1\n",
    "    keywords_hot_cold[i] = row\n",
    "\n",
    "\n",
    "#doing hot and cold encoding for locations\n",
    "locations = {}\n",
    "count = 0\n",
    "for i in df[\"location\"]:\n",
    "    if (i not in locations.keys()):\n",
    "        locations[i] = count\n",
    "        count+=1\n",
    "location_hot_cold = np.zeros((len(df[\"location\"]), len(locations.keys())))\n",
    "for i in range(location_hot_cold.shape[0]):\n",
    "    row = np.zeros(location_hot_cold.shape[1])\n",
    "    row[locations[df.at[i, \"location\"]]] = 1\n",
    "    location_hot_cold[i] = row\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4567, 24949) (4567,) (3046, 24949) (3046,)\n"
     ]
    }
   ],
   "source": [
    "# combine all features and split into training and validation sets\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), keywords_hot_cold, location_hot_cold, X))\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X, df[\"target\"], test_size=.4)\n",
    "poly = PolynomialFeatures(1)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_val = poly.fit_transform(X_val)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regression(X,y,lambda1): # We are using X for the design matrix.  It might be that X is in Z-space\n",
    "    d = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    I_prime = np.identity(d)\n",
    "    I_prime[0,0] = 0\n",
    "    w = np.linalg.inv(X.T.dot(X)+(N*lambda1)*I_prime).dot(X.T.dot(y))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39617622 0.         0.13289267 ... 0.06740625 0.         0.2114655 ]\n"
     ]
    }
   ],
   "source": [
    "w = poly_regression(X_train, y_train, .001)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3046\n",
      "0.17477143550920526\n"
     ]
    }
   ],
   "source": [
    "yhat_val = X_val.dot(w)\n",
    "E_val = (1/yhat_val.shape[0])*(np.sum((yhat_val - y_val)**2))\n",
    "print(yhat_val.shape[0])\n",
    "print(E_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
