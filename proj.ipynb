{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "# run this in cmd to avoid an error beforehand for this import\n",
    "# python -m nltk.downloader stopwords \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN       deeds reason earthquake may allah forgive us   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  residents asked shelter place notified officer...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_urls(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randNoun(lines):\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(lines)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "    if (len(nouns) == 0): # if no nouns picked up return rand word in text\n",
    "        return lines[random.randrange(len(lines))]\n",
    "    return nouns[random.randrange(len(nouns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "# preprocessing of the dataset\n",
    "print(len(df[\"location\"]))\n",
    "words = {} # will hold unique keywords\n",
    "count = 0\n",
    "# getting rid of %20 in keyword and getting a list of all the unqiue keywords\n",
    "for i in df[\"keyword\"]:\n",
    "    i = str(i).replace(\"%20\", ' ')\n",
    "    if (i not in words.keys() and i != \"nan\"):\n",
    "        words[i] = count\n",
    "        count+=1\n",
    "\n",
    "# filling in missing feature values\n",
    "# missing keywords get filled in with a keywords already in \"words\" if that keyword shows up in text, otherwise pick a random word from text\n",
    "# Missing \"location\" gets filled in with \"Earth\"\n",
    "for i,j in df.iterrows():\n",
    "    if (pd.isna(j[\"keyword\"])):\n",
    "        for k in words.keys():\n",
    "            if k in j[\"text\"]:\n",
    "                df.at[i, \"keyword\"] = k\n",
    "            else:\n",
    "                df.at[i, \"keyword\"] = randNoun(j[\"text\"])\n",
    "    if (pd.isna(j[\"location\"])):\n",
    "        df.at[i, \"location\"] = \"Earth\"\n",
    "# for i, j in df.iterrows():\n",
    "#     if (pd.isna(j[\"location\"])):\n",
    "#         df.drop(i, axis=0, inplace=True)\n",
    "\n",
    "print(len(df[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(inplace=True, drop=True)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deeds': 0, 'canada': 1, 'orders': 2, 'wildfires': 3, 'alaska': 4, 'fire': 5, 'flood': 6, 'emergency': 7, 'im': 8, 'heat': 9, 'south': 10, 'count': 11, 'damage': 12, 'whats': 13, 'fruits': 14, 'summer': 15, 'car': 16, 'goooooooaaaaaal': 17, 'i': 18, 'cool': 19, 'love': 20, 'day': 21, 'looooool': 22, 'wayi': 23, 'week': 24, 'girlfriend': 25, 'cooool': 26, 'pasta': 27, 'end': 28, 'ablaze': 29, 'accident': 30, 'aftershock': 31, 'airplane%20accident': 32, 'ambulance': 33, 'annihilated': 34, 'annihilation': 35, 'apocalypse': 36, 'armageddon': 37, 'army': 38, 'arson': 39, 'arsonist': 40, 'attack': 41, 'attacked': 42, 'avalanche': 43, 'battle': 44, 'bioterror': 45, 'bioterrorism': 46, 'blaze': 47, 'blazing': 48, 'bleeding': 49, 'blew%20up': 50, 'blight': 51, 'blizzard': 52, 'blood': 53, 'bloody': 54, 'blown%20up': 55, 'body%20bag': 56, 'body%20bagging': 57, 'body%20bags': 58, 'bomb': 59, 'bombed': 60, 'bombing': 61, 'bridge%20collapse': 62, 'buildings%20burning': 63, 'buildings%20on%20fire': 64, 'burned': 65, 'burning': 66, 'burning%20buildings': 67, 'bush%20fires': 68, 'casualties': 69, 'casualty': 70, 'catastrophe': 71, 'catastrophic': 72, 'chemical%20emergency': 73, 'cliff%20fall': 74, 'collapse': 75, 'collapsed': 76, 'collide': 77, 'collided': 78, 'collision': 79, 'crash': 80, 'crashed': 81, 'crush': 82, 'crushed': 83, 'curfew': 84, 'cyclone': 85, 'danger': 86, 'dead': 87, 'death': 88, 'deaths': 89, 'debris': 90, 'deluge': 91, 'deluged': 92, 'demolish': 93, 'demolished': 94, 'demolition': 95, 'derail': 96, 'derailed': 97, 'derailment': 98, 'desolate': 99, 'desolation': 100, 'destroy': 101, 'destroyed': 102, 'destruction': 103, 'detonate': 104, 'detonation': 105, 'devastated': 106, 'devastation': 107, 'disaster': 108, 'displaced': 109, 'drought': 110, 'drown': 111, 'drowned': 112, 'drowning': 113, 'dust%20storm': 114, 'earthquake': 115, 'electrocute': 116, 'electrocuted': 117, 'emergency%20plan': 118, 'emergency%20services': 119, 'engulfed': 120, 'epicentre': 121, 'evacuate': 122, 'evacuated': 123, 'evacuation': 124, 'explode': 125, 'exploded': 126, 'explosion': 127, 'eyewitness': 128, 'famine': 129, 'fatal': 130, 'fatalities': 131, 'fatality': 132, 'fear': 133, 'fire%20truck': 134, 'first%20responders': 135, 'flames': 136, 'flattened': 137, 'flooding': 138, 'floods': 139, 'forest%20fire': 140, 'forest%20fires': 141, 'hail': 142, 'hailstorm': 143, 'harm': 144, 'hazard': 145, 'hazardous': 146, 'heat%20wave': 147, 'hellfire': 148, 'hijack': 149, 'hijacker': 150, 'hijacking': 151, 'hostage': 152, 'hostages': 153, 'hurricane': 154, 'injured': 155, 'injuries': 156, 'injury': 157, 'inundated': 158, 'inundation': 159, 'landslide': 160, 'lava': 161, 'lightning': 162, 'loud%20bang': 163, 'mass%20murder': 164, 'mass%20murderer': 165, 'massacre': 166, 'mayhem': 167, 'meltdown': 168, 'military': 169, 'mudslide': 170, 'natural%20disaster': 171, 'nuclear%20disaster': 172, 'nuclear%20reactor': 173, 'obliterate': 174, 'obliterated': 175, 'obliteration': 176, 'oil%20spill': 177, 'outbreak': 178, 'pandemonium': 179, 'panic': 180, 'panicking': 181, 'police': 182, 'quarantine': 183, 'quarantined': 184, 'radiation%20emergency': 185, 'rainstorm': 186, 'razed': 187, 'refugees': 188, 'rescue': 189, 'rescued': 190, 'rescuers': 191, 'riot': 192, 'rioting': 193, 'rubble': 194, 'ruin': 195, 'sandstorm': 196, 'screamed': 197, 'screaming': 198, 'screams': 199, 'seismic': 200, 'sinkhole': 201, 'sinking': 202, 'siren': 203, 'sirens': 204, 'smoke': 205, 'snowstorm': 206, 'storm': 207, 'stretcher': 208, 'structural%20failure': 209, 'suicide%20bomb': 210, 'suicide%20bomber': 211, 'suicide%20bombing': 212, 'sunk': 213, 'survive': 214, 'survived': 215, 'survivors': 216, 'terrorism': 217, 'terrorist': 218, 'threat': 219, 'thunder': 220, 'thunderstorm': 221, 'tornado': 222, 'tragedy': 223, 'trapped': 224, 'trauma': 225, 'traumatised': 226, 'trouble': 227, 'tsunami': 228, 'twister': 229, 'typhoon': 230, 'upheaval': 231, 'violent%20storm': 232, 'volcano': 233, 'war%20zone': 234, 'weapon': 235, 'weapons': 236, 'whirlwind': 237, 'wild%20fires': 238, 'wildfire': 239, 'windstorm': 240, 'wounded': 241, 'wounds': 242, 'wreck': 243, 'wreckage': 244, 'wrecked': 245, 'gameofkittens': 246, 'sismo': 247, 'saudi': 248, 'omg': 249, 'weather': 250, 'year': 251, 'headquarters': 252, 'bang': 253, 'smells': 254, 'shelby': 255, 'cases': 256, 'reunion': 257, 'california': 258, 'spill': 259, 'case': 260, 'crews': 261, 'stay': 262, 'kills': 263, 'state': 264, 'hawaii': 265, 'homes': 266}\n",
      "[[  0.]\n",
      " [  1.]\n",
      " [  2.]\n",
      " ...\n",
      " [265.]\n",
      " [ 16.]\n",
      " [266.]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Earth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[259], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m locations_vector \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# column vector to store possible values\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(locations_vector\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 40\u001b[0m     locations_vector[i] \u001b[38;5;241m=\u001b[39m \u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# find corresponding value of that keyword in the \"words\" dict and assign that value \u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(locations_vector)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Earth'"
     ]
    }
   ],
   "source": [
    "# we are going to feature transform \n",
    "corpus = df[\"text\"].to_list() # combine all text to a bag of words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray() # TF-IDF text encoding\n",
    "\n",
    "#doing hot and cold encoding for keywords\n",
    "words = {} # will hold unique keywords with a corresponding index\n",
    "count = 0\n",
    "for i in df[\"keyword\"]:\n",
    "    if (i not in words.keys()):\n",
    "        words[i] = count\n",
    "        count+=1\n",
    "print(words)\n",
    "# keywords_hot_cold = np.zeros((len(df[\"keyword\"]), len(words.keys()))) # init feature shape\n",
    "# for i in range(keywords_hot_cold.shape[0]):\n",
    "#     row = np.zeros(keywords_hot_cold.shape[1])\n",
    "#     row[words[df.at[i, \"keyword\"]]] = 1 # using the index found in the dict, index the row and set the corresponding value of the keyword to 1\n",
    "#     keywords_hot_cold[i] = row\n",
    "\n",
    "keywords_vector =  np.zeros((len(df[\"keyword\"]), 1)) # column vector to store possible values\n",
    "for i in range(keywords_vector.shape[0]):\n",
    "    keywords_vector[i] = words[df.at[i, \"keyword\"]] # find corresponding value of that keyword in the \"words\" dict and assign that value \n",
    "print(keywords_vector)\n",
    "\n",
    "#doing hot and cold encoding for locations\n",
    "locations = {}\n",
    "count = 0\n",
    "for i in df[\"location\"]:\n",
    "    if (i not in locations.keys()):\n",
    "        locations[i] = count\n",
    "        count+=1\n",
    "# location_hot_cold = np.zeros((len(df[\"location\"]), len(locations.keys())))\n",
    "# for i in range(location_hot_cold.shape[0]):\n",
    "#     row = np.zeros(location_hot_cold.shape[1])\n",
    "#     row[locations[df.at[i, \"location\"]]] = 1\n",
    "#     location_hot_cold[i] = row\n",
    "\n",
    "locations_vector =  np.zeros((len(df[\"location\"]), 1)) # column vector to store possible values\n",
    "for i in range(locations_vector.shape[0]):\n",
    "    locations_vector[i] = words[df.at[i, \"location\"]] # find corresponding value of that keyword in the \"words\" dict and assign that value \n",
    "print(locations_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 267)\n",
      "(7613, 3341)\n"
     ]
    }
   ],
   "source": [
    "print(keywords_hot_cold.shape)\n",
    "print(location_hot_cold.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4567, 21323) (4567,) (3046, 21323) (3046,)\n"
     ]
    }
   ],
   "source": [
    "# combine all features and split into training and validation sets\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), keywords_hot_cold, location_hot_cold, X))\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X, df[\"target\"], test_size=.4)\n",
    "# poly = PolynomialFeatures(1)\n",
    "# X_train = poly.fit_transform(X_train)\n",
    "# X_val = poly.fit_transform(X_val)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regression(X,y,lambda1): # We are using X for the design matrix.  It might be that X is in Z-space\n",
    "    d = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "    I_prime = np.identity(d)\n",
    "    I_prime[0,0] = 0\n",
    "    w = np.linalg.inv(X.T.dot(X)+(N*lambda1)*I_prime).dot(X.T.dot(y))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[256], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mpoly_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(w)\n",
      "Cell \u001b[1;32mIn[255], line 6\u001b[0m, in \u001b[0;36mpoly_regression\u001b[1;34m(X, y, lambda1)\u001b[0m\n\u001b[0;32m      4\u001b[0m I_prime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(d)\n\u001b[0;32m      5\u001b[0m I_prime[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m(N\u001b[38;5;241m*\u001b[39mlambda1)\u001b[38;5;241m*\u001b[39mI_prime)\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(y))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w = poly_regression(X_train, y_train, .001)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3046\n",
      "0.17477143550920526\n"
     ]
    }
   ],
   "source": [
    "yhat_val = X_val.dot(w)\n",
    "E_val = (1/yhat_val.shape[0])*(np.sum((yhat_val - y_val)**2))\n",
    "print(yhat_val.shape[0])\n",
    "print(E_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
